# API Documentation

## Base URL

```
http://localhost:8000/api/v1
```

## Authentication

No authentication required for this version.

## Endpoints

### 1. Health Check

Check Ollama connectivity and model availability.

**Endpoint:** `GET /health`

**Response:**

```json
{
  "status": "healthy",
  "ollama_available": true,
  "model_available": true,
  "configured_model": "llama3.1:8b",
  "available_models": ["llama3.1:8b", "mistral:7b"]
}
```

**Status Values:**
- `healthy`: Ollama is running and configured model is available
- `degraded`: Ollama is running but configured model is not available
- `unhealthy`: Cannot connect to Ollama

**Example:**

```bash
curl http://localhost:8000/api/v1/health
```

---

### 2. Send Chat Message (Non-Streaming)

Send a message and receive a complete response.

**Endpoint:** `POST /chat`

**Request Body:**

```json
{
  "session_id": "string",
  "message": "string",
  "include_context": true
}
```

**Parameters:**
- `session_id` (required): Unique identifier for conversation continuity
- `message` (required): User's message to the assistant
- `include_context` (optional, default: true): Whether to include questionnaire context in the response

**Response:**

```json
{
  "session_id": "user-123",
  "message": "Thank you for sharing that information. Let me ask you about...",
  "timestamp": "2024-01-15T10:30:00Z"
}
```

**Example:**

```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "accounting-firm-001",
    "message": "I run a small accounting firm with 5 employees. We use QuickBooks Online and store client data in Excel files.",
    "include_context": true
  }'
```

**Error Response (500):**

```json
{
  "detail": "Failed to process chat message. Please ensure Ollama is running. Error: ..."
}
```

---

### 3. Send Chat Message (Streaming)

Send a message and receive a streaming response in real-time.

**Endpoint:** `POST /chat/stream`

**Request Body:** Same as non-streaming chat

**Response:** Server-Sent Events (SSE)

**Event Types:**

1. **Message Chunk:**
```
event: message
data: {"content": "text chunk"}
```

2. **Completion:**
```
event: done
data: {"status": "complete"}
```

3. **Error:**
```
event: error
data: {"error": "error message"}
```

**Example (Python):**

```python
import httpx
import json

async with httpx.AsyncClient(timeout=30.0) as client:
    async with client.stream(
        "POST",
        "http://localhost:8000/api/v1/chat/stream",
        json={
            "session_id": "user-123",
            "message": "What are backup best practices?",
            "include_context": True,
        }
    ) as response:
        async for line in response.aiter_lines():
            if line.startswith("data: "):
                data = json.loads(line[6:])
                if "content" in data:
                    print(data["content"], end="", flush=True)
```

**Example (curl):**

```bash
curl -N -X POST http://localhost:8000/api/v1/chat/stream \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "user-123",
    "message": "Tell me about cloud security"
  }'
```

---

### 4. Get Chat History

Retrieve conversation history for a specific session.

**Endpoint:** `GET /chat/history/{session_id}`

**Parameters:**
- `session_id` (path parameter): The session identifier

**Response:**

```json
{
  "session_id": "user-123",
  "messages": [
    {
      "role": "user",
      "content": "Hello, I need help with cybersecurity",
      "timestamp": "2024-01-15T10:00:00Z"
    },
    {
      "role": "assistant",
      "content": "Hi! I'd be happy to help you with cybersecurity...",
      "timestamp": "2024-01-15T10:00:02Z"
    }
  ]
}
```

**Message Roles:**
- `user`: Messages from the user
- `assistant`: Messages from the AI assistant

**Example:**

```bash
curl http://localhost:8000/api/v1/chat/history/user-123
```

**Empty History:**

```json
{
  "session_id": "new-session",
  "messages": []
}
```

---

## Session Management

### Session IDs

Session IDs are used to maintain conversation context across multiple requests. They should be:
- Unique per user or conversation
- Persistent across page reloads
- Generated by the client

**Recommended Format:**
- UUID v4: `550e8400-e29b-41d4-a716-446655440000`
- User-based: `user-{user_id}-{timestamp}`
- Context-based: `business-{business_id}`

**Example Session ID Generation (JavaScript):**

```javascript
const sessionId = `session-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
```

**Example Session ID Generation (Python):**

```python
import uuid
session_id = str(uuid.uuid4())
```

---

## Conversation Memory

The system maintains conversation history in a database. Each session includes:

1. **Chat History**: All user and assistant messages
2. **Questionnaire Context**: Previous questionnaire responses (if available)
3. **Timestamps**: When each message was sent

**Memory Persistence:**
- Chat history is stored indefinitely
- Use the same `session_id` to continue a conversation
- Use a new `session_id` to start a fresh conversation

---

## Context Retrieval

When `include_context: true` is set, the system:

1. Retrieves prior questionnaire responses for the session
2. Includes them in the system prompt
3. Enables more specific and relevant follow-up questions

**Example Workflow:**

1. User completes questionnaire â†’ Responses stored with `session_id`
2. User starts chat with same `session_id`
3. Assistant asks intelligent follow-ups based on questionnaire responses

---

## Rate Limiting

No rate limiting is currently implemented. For production use, consider:
- Rate limiting per IP address
- Concurrent request limits per session
- Request queuing for CPU-bound inference

---

## Error Handling

### Common Error Scenarios

1. **Ollama Not Running**
   - Status: 500
   - Message: "Failed to process chat message. Please ensure Ollama is running."
   - Solution: Start Ollama with `ollama serve`

2. **Model Not Available**
   - Check health endpoint first
   - Status: "degraded" if model missing
   - Solution: Pull model with `ollama pull llama3.1:8b`

3. **Invalid Request**
   - Status: 422 (Validation Error)
   - Response includes field-specific error messages

4. **Database Error**
   - Status: 500
   - Check database file permissions and disk space

---

## Response Times

**Expected Response Times (CPU-only, llama3.1:8b):**

- Health check: <100ms
- Chat (non-streaming): 2-5 seconds
- Chat (streaming): First chunk in ~500ms, then continuous
- Chat history: <100ms

**Factors Affecting Response Time:**
- First request loads model into memory (slower)
- Message length
- Conversation history length
- System resources (CPU, RAM)

---

## Interactive Documentation

FastAPI provides automatic interactive documentation:

### Swagger UI
```
http://localhost:8000/docs
```
- Interactive API explorer
- Test endpoints directly in browser
- View request/response schemas

### ReDoc
```
http://localhost:8000/redoc
```
- Alternative documentation format
- Better for reading and sharing

### OpenAPI Schema
```
http://localhost:8000/openapi.json
```
- Machine-readable API specification
- Use with API client generators

---

## Example Use Cases

### 1. Simple Q&A

```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "demo-001",
    "message": "What is the NIST Cybersecurity Framework?"
  }'
```

### 2. Multi-Turn Conversation

```bash
# First message
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "demo-002",
    "message": "I use Microsoft 365 for email and file storage."
  }'

# Follow-up (uses same session_id)
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "demo-002",
    "message": "What security features should I enable?"
  }'
```

### 3. Context-Aware Follow-Up

Assuming questionnaire responses exist for session "business-123":

```bash
curl -X POST http://localhost:8000/api/v1/chat \
  -H "Content-Type: application/json" \
  -d '{
    "session_id": "business-123",
    "message": "Can you help me improve our security based on my previous answers?",
    "include_context": true
  }'
```

---

## Best Practices

1. **Session Management**
   - Use meaningful session IDs
   - Store session IDs client-side (localStorage, cookies)
   - Don't expose session IDs in URLs

2. **User Experience**
   - Use streaming for real-time feedback
   - Show loading indicators during requests
   - Handle errors gracefully with retry logic

3. **Performance**
   - Cache health check results (1-minute TTL)
   - Limit message length (< 2000 chars)
   - Implement request timeouts (30 seconds)

4. **Security**
   - Sanitize user input
   - Implement authentication for production
   - Use HTTPS in production
   - Validate session IDs server-side

---

## Client SDKs

### Python

```python
import httpx
import asyncio

class NISTChatClient:
    def __init__(self, base_url="http://localhost:8000/api/v1"):
        self.base_url = base_url
    
    async def health(self):
        async with httpx.AsyncClient() as client:
            response = await client.get(f"{self.base_url}/health")
            return response.json()
    
    async def chat(self, session_id, message, include_context=True):
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                f"{self.base_url}/chat",
                json={
                    "session_id": session_id,
                    "message": message,
                    "include_context": include_context,
                }
            )
            return response.json()
    
    async def get_history(self, session_id):
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{self.base_url}/chat/history/{session_id}"
            )
            return response.json()

# Usage
client = NISTChatClient()
response = await client.chat("user-123", "Hello!")
```

### JavaScript

```javascript
class NISTChatClient {
  constructor(baseUrl = 'http://localhost:8000/api/v1') {
    this.baseUrl = baseUrl;
  }
  
  async health() {
    const response = await fetch(`${this.baseUrl}/health`);
    return response.json();
  }
  
  async chat(sessionId, message, includeContext = true) {
    const response = await fetch(`${this.baseUrl}/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        session_id: sessionId,
        message: message,
        include_context: includeContext
      })
    });
    return response.json();
  }
  
  async getHistory(sessionId) {
    const response = await fetch(
      `${this.baseUrl}/chat/history/${sessionId}`
    );
    return response.json();
  }
}

// Usage
const client = new NISTChatClient();
const response = await client.chat('user-123', 'Hello!');
```

---

## Support

For issues or questions:
- Check the [README.md](README.md) for setup instructions
- Review the [SETUP.md](SETUP.md) for troubleshooting
- Test with the example script: `python examples/chat_example.py`
